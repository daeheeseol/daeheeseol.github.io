---
title: Feature Selection 이란?
description: Feature Selection에 대해 알아보기
author: SemiDS
date: 2025-02-16 12:00:00 +0900
categories: [Data Science, Machine Learning]
tags: [Feature Selection]
pin: true
---

## Feature Selection 이란?
**Feature Selection (특성 선택)**은 기계학습 모델의 성능을 향상시키고 계산 비용을 줄이기 위해 Raw Data에서 중요한 특성만 선택해서 사용하는 방법입니다. Raw Data의 독립 변수(X, Feature)간에 중복 또는 상관성이 높은 변수가 포함되어 있거나[`다중공선성`], 종속 변수(Y, Label)와 관련이 없는 특성이 있는 경우 이를 제외하고 모델을 구축해 성능을 높이기 위한 접근 방법이라고 할 수 있습니다.  

이를 위해 주로 사용되는 대표적인 접근 방법은 `Filter`, `Wrapper`, `Embedded Method`가 있으며, Data의 특성과 목적에 따라 적절한 방법을 선택해서 사용하게 됩니다.

**1. Filter Method**   
\- 통계적인 기법을 사용해서 특성의 중요도를 평가해서 선택하는 방법
- Variance Threshold: 분산이 특정 Thresold 보다 낮은 feature 제거
- Correlation Coefficient: 상관 계수가 높은 feature 제거
- Chi-square Test: 범주형 데이터에서 중요 feature 선택
- Information Gain & Gini Coefficient: 엔트로피 기반 정보 측정
- Fisher's Score: 평균과 분산을 기준으로 중요 feature 선택

***

**2. Wrapper Method**  
\- 다양한 특성 조합으로 모델을 학습시켜 가장 좋은 성능을 보이는 최적의 특성 조합(Subset)을 찾아내는 방법
- Forward Selection: 특성이 없는 상태부터 시작해 성능이 가장 좋은 특성부터 추가하는 방법
- Backward Selection: 모든 특성부터 시작해, 중요하지 않은 특성을 하나씩 제거하는 방법
- Stepwise Selection: Forward & Backward Selection을 적절하게 결합해서 진행하는 방법

***

**3. Embedded Method**  
\- Filter & Wrapper Method의 장점을 결합한 방법으로, 모델 알고리즘이 학습을 하면서 자체적으로 특성을 선택하는 방법
- Lasso Regression : L1 regularization을 통해 중요도가 낮은 특성의 가중치를 0으로 만드는 방법
- Ridge Regression : L2 regularization를 통해 중요도가 낮은 특성의 가중치를 0에 가깝게 만드는 방법
- Elastic Net: L1 + L2 regularization를 조합하여 특성을 선택하는 방법
- Select From Model: Random Forest나 XGBoost 같은 Tree-based model의 Feature Importance를 활용하는 방법

<br>

-참고 Article
>[https://delphinus.tistory.com/category/Machine%20Learning](https://delphinus.tistory.com/category/Machine%20Learning)
